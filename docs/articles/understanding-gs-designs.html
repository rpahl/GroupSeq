<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Understanding group sequential designs • GroupSeq</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/sandstone/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Understanding group sequential designs">
<meta property="og:description" content="See this vignette if you want to understand how group sequential design probabilities are calculated and gain an intuitive understanding of the underlying methods.
">
<meta property="og:image" content="https://github.com/rpahl/GroupSeq/logo.svg">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">GroupSeq</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">1.4.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/GroupSeq.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a></a>
    </li>
    <li>
      <a href="../articles/index.html">Overview</a>
    </li>
    <li class="divider">
    <li>
      <a href="../articles/understanding-gs-designs.html">Understanding group sequential designs</a>
    </li>
    <li>
      <a href="../articles/GroupSeq.html">Introduction to GroupSeq</a>
    </li>
    <li>
      <a href="../articles/1-compute-bounds-H0.html">-1- Compute Bounds</a>
    </li>
    <li>
      <a href="../articles/2-compute-drift.html">-2- Compute Drift given Power and Bounds</a>
    </li>
    <li>
      <a href="../articles/3-compute-bounds-H1.html">-3- Compute Probabilities given Bounds and Drift</a>
    </li>
    <li>
      <a href="../articles/4-compute-CI.html">-4- Compute Confidence Interval</a>
    </li>
    <li class="divider">
    <li class="dropdown-header">Other</li>
    <li>
      <a href="https://www.r-project.org/doc/Rnews/Rnews_2006-2.pdf#page=21">GroupSeq article in R News 2006</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    My Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="https://rpahl.github.io/container/">container</a>
    </li>
    <li>
      <a href="https://rpahl.github.io/GroupSeq/">GroupSeq</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rpahl/GroupSeq/">
    <span class="fab fa-github"></span>
     
  </a>
</li>
<li>
  <a href="https://stackoverflow.com/users/8120617/rpahl">
    <span class="fab fa-stack-overflow"></span>
     
  </a>
</li>
<li>
  <a href="https://twitter.com/romanpahl">
    <span class="fab fa-twitter"></span>
     
  </a>
</li>
<li>
  <a href="https://rpahl.github.io/r-some-blog/">
    <span class="fas fa-blog"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="understanding-gs-designs_files/header-attrs-2.11/header-attrs.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Understanding group sequential designs</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rpahl/GroupSeq/blob/master/R/vignettes/understanding-gs-designs.Rmd"><code>R/vignettes/understanding-gs-designs.Rmd</code></a></small>
      <div class="hidden name"><code>understanding-gs-designs.Rmd</code></div>

    </div>

    
    
<p>While calculating probabilities of group sequential designs can be considered somewhat complex, it may be less difficult than most people think. This vignette provides a visual explanation of the underlying method to help forming an intuitive understanding of this topic. The author is convinced that gaining an intuitive understanding of these calculations helps when applying and interpreting such designs in practice.</p>
<div id="one-sample-study" class="section level3">
<h3 class="hasAnchor">
<a href="#one-sample-study" class="anchor"></a>One-sample study</h3>
<p>Following the easy-to-hard principle, we start with conventional non-sequential study designs and gradually progress to the group sequential case.</p>
<p>Consider you have a sample of data, let’s call it <code>X1</code>.</p>
<div class="figure" style="text-align: center">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-2-1.png" alt="Schematic view of a study sample - each circle represents some measurement." width="35%"><p class="caption">
Schematic view of a study sample - each circle represents some measurement.
</p>
</div>
<p><br> Usually some standardized test statistic is calculated and, assuming standard normally distributed data, the density distribution under the null hypothesis forms the well-known bell-shaped curve.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-3-1.png" alt="Standard Normal Density with critial bounds marking 5% significance level." width="65%"><p class="caption">
Standard Normal Density with critial bounds marking 5% significance level.
</p>
</div>
<p><br> If a two-sided test is planned with a significance level of <span class="math inline">\(\alpha = 5\%\)</span>, the critical bounds are set as visualized in the above figure. In the days before computers these bounds had to be looked up in tables in some book, but these days of course you just plug-in the numbers in your favorite statistics program. In R its</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.025</span><span class="op">)</span>
<span class="co"># [1] -1.959963985</span></code></pre></div>
<p>Now what do these bounds mean? Basically, if the null hypothesis (<span class="math inline">\(H_0\)</span>) was true, the probability of obtaining a test statistic at or beyond these bounds is <span class="math inline">\(2\cdot 0.025 = 0.05\)</span>, which therefore limits the so-called type I error (rejecting <span class="math inline">\(H_0\)</span> although its true) at 5%.</p>
<p>Going forward the key thing to note here is that calculating this probability basically means <em>calculating the (blue) area under the density curve</em> (see above figure). Again, these days it’s easy to derive the area using computer’s numerical integration.</p>
</div>
<div id="combining-two-studies" class="section level3">
<h3 class="hasAnchor">
<a href="#combining-two-studies" class="anchor"></a>Combining two studies</h3>
<p>Now lets consider another sample <code>X2</code> that was collected independently from <code>X1</code>.</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-5-1.png" width="35%" style="display: block; margin: auto;"></p>
<p><br> If we construct a distribution similar to the one above, we have the following.</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-6-1.png" width="85%"></p>
<p><br> A statistical test on both samples now would control the type I error for each of these studies at the 5% level, but, of course, things are more complicated if you consider both samples at the same time, because now you have two chances of conducting the type I error.</p>
<p>So if we assume our samples <code>X1</code> and <code>X2</code> were sampled from the same population and we are interested to test the same outcome, to control the <em>overall</em> type I error still at 5% we need to down-adjust alpha. The simplest and most common approach is to use the Bonferroni correction, which means we are allowed to spend <span class="math inline">\(\alpha_{Bonf} = \alpha/2 = 0.025\)</span>. The corresponding adjusted critical bound is</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.025</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span>
<span class="co"># [1] -2.241402728</span></code></pre></div>
<p>and we have</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-9-1.png" width="85%"></p>
<p>So if we calculate the overall type I error probability, we sum up all the areas and get <span class="math inline">\(p = 1.25\% + 1.25\%+ 1.25\% + 1.25\% = 5\%\)</span>. So we are good, are we? Well, the devil is in the details.</p>
<p>Remember, the goal was to make sure that we don’t conduct a type I error. The probability to conduct <em>no</em> type I at either <code>X1</code> or <code>X2</code> is <span class="math inline">\(p = 1-\alpha_{Bonf} = 1-0.025 = 0.975\)</span>.</p>
<p>From this it follows that the probability to conduct no error at both <code>X1</code> and <code>X2</code> is <span class="math inline">\((1-\alpha_{Bonf})^2 = 0.950625\)</span>.</p>
<p>Now we can calculate the complementary probability of conducting <em>at least one</em> type I error as <span class="math inline">\(p = 1 - (1-\alpha_{Bonf})^2 = 0.049375 = 4.9\%\)</span>. Wait, why is it not 5%? Well, the reason is that we don’t care if one or both studies produce a type I error, as one error is already enough for the overall setup to fail. As a result, the Bonferroni correction is not optimal, as it “wastes” 0.1% of alpha.</p>
<p><br> Of course, this is well-known and a precise alpha adjustment can be derived via the so-called <a href="https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction">Sidak correction</a>: <span class="math inline">\(\alpha_{Sidak} = 1 - (1-\alpha)^{1/2} = 0.02532057 &gt; \alpha_{Bonf}\)</span>.</p>
<p>Plugging this into the above formula, we get <span class="math inline">\(p = 1 - (1-\alpha_{Sidak})^2 = 0.05 = 5\%\)</span>. If you are interested, you can find the derivation of <a href="https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction">Sidak’s</a> formula under the link, but I have promised you to provide intuitive explanations, so let’s move to the key concept that will also help to understand group sequential probabilities.</p>
</div>
<div id="combining-two-distributions" class="section level3">
<h3 class="hasAnchor">
<a href="#combining-two-distributions" class="anchor"></a>Combining two distributions</h3>
<p>If we want to combine both studies, it makes sense to combine the two density distributions into a two-dimensional (aka bivariate) normal distribution, which then looks like this.</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-11-1.png" width="55%"></p>
<p>Remember that determining the probability of distributions is all about doing numerical integration under the curve, that is, the overall type I error now is derived by computing the volume under the blue area. To do this, we are using the <a href="https://CRAN.R-project.org/package=mvtnorm">mvtnorm</a> package.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>If we plug in the Bonferroni bounds (2.2414) in each dimension and calculate the probability for the blue area, we get p = 0.049375 while the Sidak bounds (2.2365) yield p = 0.05, which matches our results from above.</p>
<p>However, we now also have a graphical intuitive interpretation to explain the results. For this, have a look at the next graph.</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-13-1.png" width="55%"></p>
<p>The grey areas basically mark those scenarios where the type I errors of both samples fall together. The intuitive explanation now goes like this: when calculating the overall probability using the two-dimensional normal distribution, the grey areas are only counted once whereas in the one-dimensional Bonferroni case you count them twice. To see this, we can calculate the probability for one of these edges, which is p = 0.00016, and if we subtract them from the overall alpha we get <span class="math inline">\(p = 0.05 - 4\cdot 0.00016 = 0.04936\)</span>, which corresponds to the overall probability under Bonferroni correction as we saw above.</p>
<p>So when using the two-dimensional normal distribution, we naturally derive at the optimal solution for the alpha adjustment. The same is true for group sequential designs, which further optimize the calculation by taking into account the dependency of the staged samples.</p>
</div>
<div id="group-sequential-study" class="section level3">
<h3 class="hasAnchor">
<a href="#group-sequential-study" class="anchor"></a>Group sequential study</h3>
<p>In a (group) sequential study design, samples are analyzed in a sequence where at each stage all the data from earlier stages are combined with the data of the current stage.</p>
<div class="figure" style="text-align: center">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-14-1.png" alt="Schematic view of group sequential two-stage study." width="35%"><p class="caption">
Schematic view of group sequential two-stage study.
</p>
</div>
<p>The crucial difference when combining the samples in this way is that now they are inherently dependent, that is, the statistic of sample <code>X1 + X2</code> will be correlated to the that of sample <code>X1</code>.</p>
<p>The naive approach for the alpha adjustment again would be to treat both density distributions as independent and apply the Bonferroni correction.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-15-1.png" alt="Normal densities for stage 1 and 2 with Bonferroni bounds." width="85%"><p class="caption">
Normal densities for stage 1 and 2 with Bonferroni bounds.
</p>
</div>
<p>Since this completely ignores the correlation, we expect that even more alpha is wasted. To see this let’s now again use the two-dimensional distribution.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-17-1.png" alt="Bivariate normal density of two-stage design viewed from different angles." width="30%"><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-17-2.png" alt="Bivariate normal density of two-stage design viewed from different angles." width="30%"><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-17-3.png" alt="Bivariate normal density of two-stage design viewed from different angles." width="30%"><p class="caption">
Bivariate normal density of two-stage design viewed from different angles.
</p>
</div>
<p>We again omit the details on how to derive the distribution, but as you can see, the correlation leads to a flattened bell curve, which in the independent case before was perfectly symmetrical. Also the amount of correlation is proportional to the data overlap of the stages. For example, if the first stage used 5 instead of the 10 data points, the stages would be less correlated and therefore the bell curve less flattened.</p>
<p>Based on this distribution, the Bonferroni bounds (2.24) now yield p = 0.043 so that for this group sequential study 0.7% of alpha would be “wasted”.</p>
<p>Intuitively, the flattened curve has pulled some mass into the middle leaving less volume under the blue area.</p>
<div id="pocock-design" class="section level4">
<h4 class="hasAnchor">
<a href="#pocock-design" class="anchor"></a>Pocock design</h4>
<p>The idea of group sequential designs obviously is to come up with bounds that lead to overall 5% type I error. Let’s start with so-called Pocock bounds, which means we want equal bounds at all stages. Using the GroupSeq package, these can be derived as follows.</p>
<p><img src="figures/two-stage-Pocock-bounds.png" width="40%"></p>
<p><img src="figures/two-stage-Pocock-bounds-result.png" width="70%"></p>
<p>So the Pocock bounds are 2.1783 and thus as expected lower than the Bonferroni bounds. The last entry in the ‘cumulative alpha’ column shows the overall alpha, which is close to 5%.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>Inspecting the probabilities stage by stage, 2.93% of alpha is spent at the first and 2.04% at the second stage, respectively. That is, although the nominal bounds are identical at both stages, due to the asymmetric shape of the two-dimensional distribution, the resulting probabilities are different.</p>
<p>The graphical presentation of group sequential designs normally is done with a graph showing at the x-axis the “Information Rate”, which marks the relative sample size of the respective stages, and the nominal bounds at the y-axis. The above Pocock design thus looks something like this.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-22-1.png" alt="Group sequential 2-stage Pocock design with one interim look at half of the total samples." width="50%"><p class="caption">
Group sequential 2-stage Pocock design with one interim look at half of the total samples.
</p>
</div>
</div>
<div id="obrien-fleming-design" class="section level4">
<h4 class="hasAnchor">
<a href="#obrien-fleming-design" class="anchor"></a>O’Brien-Fleming design</h4>
<p>Probably the most popular group sequential design is the O’Brien-Fleming design. For our two-stage scenario with half of the sample used in each stage, it looks like this.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-23-1.png" alt="Group sequential 2-stage O'Brien-Fleming design with one interim look at 0.5." width="50%"><p class="caption">
Group sequential 2-stage O’Brien-Fleming design with one interim look at 0.5.
</p>
</div>
<p>The numbers are outlined below.</p>
<p><img src="figures/two-stage-OBF-bounds-result.png" width="70%"></p>
<p>Clearly the critical bounds at stage one are very conservative so that only 0.3% of alpha is spent there. With spending 4.7% of alpha at the last stage this O’Brien-Fleming design in fact is not much different to a standard single stage study, which spends all the 5% on the full sample. Basically, the first stage is kind of a shot in the dark to allow catching really big effects.</p>
<p>In contrast, the Pocock design appears to be more “aggressive” trying harder to terminate the study early. Of course, the drawback of the Pocock strategy is that if the study cannot be terminated after the first stage, it then only has about 2% of alpha left for the final stage.</p>
<p>To apply the O’Brien-Fleming design more aggressively, one could plan the first stage analysis at a later time point. For example, having the first interim analysis after 70% of the samples already means that about 1.5% of alpha are spent at stage 1.</p>
</div>
<div id="design-comparisons" class="section level4">
<h4 class="hasAnchor">
<a href="#design-comparisons" class="anchor"></a>Design comparisons</h4>
<p>To strengthen our intuitive understanding, let’s compare the Pocock and O’Brien-Fleming design in the two-dimensional density view.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-25-1.png" alt="Bivariate normal density with critical bounds of Pocock (left) and O'Brien-Fleming design (right) both with interim analysis after 50% of the sample." width="45%"><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-25-2.png" alt="Bivariate normal density with critical bounds of Pocock (left) and O'Brien-Fleming design (right) both with interim analysis after 50% of the sample." width="45%"><p class="caption">
Bivariate normal density with critical bounds of Pocock (left) and O’Brien-Fleming design (right) both with interim analysis after 50% of the sample.
</p>
</div>
<p>The figure below shows how the time point of the stage 1 analysis affects the correlation of the stages and with that shapes the design probabilities.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-26-1.png" alt="Bivariate normal density of two-stage O'Brien-Fleming design with first stage after 50% (left), 70% (middle) and 90% (right) of all samples." width="30%"><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-26-2.png" alt="Bivariate normal density of two-stage O'Brien-Fleming design with first stage after 50% (left), 70% (middle) and 90% (right) of all samples." width="30%"><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-26-3.png" alt="Bivariate normal density of two-stage O'Brien-Fleming design with first stage after 50% (left), 70% (middle) and 90% (right) of all samples." width="30%"><p class="caption">
Bivariate normal density of two-stage O’Brien-Fleming design with first stage after 50% (left), 70% (middle) and 90% (right) of all samples.
</p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>To keep matters simple, we just discuss the results, but if you want to see the actual R commands, just inspect the source code of this vignette.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>In fact, it is closer to 5% (0.0499966086) than shown in the GroupSeq application, which stems from the fact that the GroupSeq legacy version uses a different more rudimentary numerical integration algorithm.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p>Developed by <a href="https://rpahl.github.io/r-some-blog/about.html">Roman Pahl</a>.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
