<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Understanding group sequential designs • GroupSeq</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Understanding group sequential designs">
<meta name="description" content="See this vignette if you want to understand how group sequential design probabilities are calculated and gain an intuitive understanding of the underlying methods.
">
<meta property="og:description" content="See this vignette if you want to understand how group sequential design probabilities are calculated and gain an intuitive understanding of the underlying methods.
">
<meta property="og:image" content="https://github.com/rpahl/GroupSeq/logo.png">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">GroupSeq</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.4.4</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../index.html" aria-label="home"><span class="fa fa-home fa-lg"></span></a></li>
<li class="nav-item"><a class="nav-link" href="../articles/GroupSeq.html">Get Started</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/index.html">Overview</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../articles/understanding-gs-designs.html">Understanding group sequential designs</a></li>
    <li><a class="dropdown-item" href="../articles/GroupSeq.html">Introduction to GroupSeq</a></li>
    <li><a class="dropdown-item" href="../articles/task-1-compute-bounds-H0.html">-1- Compute Bounds</a></li>
    <li><a class="dropdown-item" href="../articles/task-2-compute-drift.html">-2- Compute Drift given Power and Bounds</a></li>
    <li><a class="dropdown-item" href="../articles/task-3-compute-bounds-H1.html">-3- Compute Probabilities given Bounds and Drift</a></li>
    <li><a class="dropdown-item" href="../articles/task-4-compute-CI.html">-4- Compute Confidence Interval</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Other</h6></li>
    <li><a class="external-link dropdown-item" href="https://www.r-project.org/doc/Rnews/Rnews_2006-2.pdf#page=21">GroupSeq article in R News 2006</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><a class="nav-link" href="https://github.com/rpahl/GroupSeq/" aria-label="github"><span class="fa fab fa-github"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://stackoverflow.com/users/8120617/rpahl" aria-label="stackoverflow"><span class="fa fab fa-stack-overflow"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://bsky.app/profile/rpahl.bsky.social" aria-label="bluesky"><span class="fa fab fa-bluesky"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://rpahl.github.io/r-some-blog/" aria-label="blog"><span class="fa fas fa-blog"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Understanding group sequential designs</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rpahl/GroupSeq/blob/HEAD/vignettes/understanding-gs-designs.Rmd"><code>vignettes/understanding-gs-designs.Rmd</code></a></small>
      <div class="d-none name"><code>understanding-gs-designs.Rmd</code></div>
    </div>

    
    
<p>While calculating probabilities of group sequential designs can be
considered somewhat complex, it may be less difficult than most people
think. This vignette provides a visual explanation of the underlying
method to help forming an intuitive understanding of this topic. The
author is convinced that gaining an intuitive understanding of these
calculations helps when applying and interpreting such designs in
practice.</p>
<div class="section level3">
<h3 id="one-sample-study">One-sample study<a class="anchor" aria-label="anchor" href="#one-sample-study"></a>
</h3>
<p>Following the easy-to-hard principle, we start with conventional
non-sequential study designs and gradually progress to the group
sequential case.</p>
<p>Consider you have a sample of data, let’s call it
<code>X1</code>.</p>
<div class="figure" style="text-align: center">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-2-1.png" alt="study-scheme" width="35%"><p class="caption">
Schematic view of a study sample - each circle represents some
measurement.
</p>
</div>
<p><br> Usually some standardized test statistic is calculated and,
assuming standard normally distributed data, the density distribution
under the null hypothesis forms the well-known bell-shaped curve.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-3-1.png" alt="standard-normal-density" width="65%"><p class="caption">
Standard Normal Density with critial bounds marking 5% significance
level.
</p>
</div>
<p><br> If a two-sided test is planned with a significance level of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>5</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">\alpha = 5\%</annotation></semantics></math>,
the critical bounds are set as visualized in the above figure. In the
days before computers these bounds had to be looked up in tables in some
book, but these days of course you just plug-in the numbers in your
favorite statistics program. In R its</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">qnorm</a></span><span class="op">(</span><span class="fl">0.025</span><span class="op">)</span></span>
<span><span class="co"># [1] -1.959963985</span></span></code></pre></div>
<p>Now what do these bounds mean? Basically, if the null hypothesis
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math>)
was true, the probability of obtaining a test statistic at or beyond
these bounds is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>⋅</mo><mn>0.025</mn><mo>=</mo><mn>0.05</mn></mrow><annotation encoding="application/x-tex">2\cdot 0.025 = 0.05</annotation></semantics></math>,
which therefore limits the so-called type I error (rejecting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math>
although its true) at 5%.</p>
<p>Going forward the key thing to note here is that calculating this
probability basically means <em>calculating the (blue) area under the
density curve</em> (see above figure). Again, these days it’s easy to
derive the area using computer’s numerical integration.</p>
</div>
<div class="section level3">
<h3 id="combining-two-studies">Combining two studies<a class="anchor" aria-label="anchor" href="#combining-two-studies"></a>
</h3>
<p>Now lets consider another sample <code>X2</code> that was collected
independently from <code>X1</code>.</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-5-1.png" alt="study-two-samples" width="35%" style="display: block; margin: auto;"></p>
<p><br> If we construct a distribution similar to the one above, we have
the following.</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-6-1.png" alt="two-normal-distributions-side-by-side" width="85%"></p>
<p><br> A statistical test on both samples now would control the type I
error for each of these studies at the 5% level, but, of course, things
are more complicated if you consider both samples at the same time,
because now you have two chances of conducting the type I error.</p>
<p>So if we assume our samples <code>X1</code> and <code>X2</code> were
sampled from the same population and we are interested to test the same
outcome, to control the <em>overall</em> type I error still at 5% we
need to down-adjust alpha. The simplest and most common approach is to
use the Bonferroni correction, which means we are allowed to spend
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mrow><mi>B</mi><mi>o</mi><mi>n</mi><mi>f</mi></mrow></msub><mo>=</mo><mi>α</mi><mi>/</mi><mn>2</mn><mo>=</mo><mn>0.025</mn></mrow><annotation encoding="application/x-tex">\alpha_{Bonf} = \alpha/2 = 0.025</annotation></semantics></math>.
The corresponding adjusted critical bound is</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">qnorm</a></span><span class="op">(</span><span class="fl">0.025</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co"># [1] -2.241402728</span></span></code></pre></div>
<p>and we have</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-9-1.png" alt="standard-normal-densities-with-adjusted-critical-bounds" width="85%"></p>
<p>So if we calculate the overall type I error probability, we sum up
all the areas and get
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1.25</mn><mi>%</mi><mo>+</mo><mn>1.25</mn><mi>%</mi><mo>+</mo><mn>1.25</mn><mi>%</mi><mo>+</mo><mn>1.25</mn><mi>%</mi><mo>=</mo><mn>5</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">p = 1.25\% + 1.25\%+ 1.25\% + 1.25\% = 5\%</annotation></semantics></math>.
So we are good, are we? Well, the devil is in the details.</p>
<p>Remember, the goal was to make sure that we don’t conduct a type I
error. The probability to conduct <em>no</em> type I at either
<code>X1</code> or <code>X2</code> is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn><mo>−</mo><msub><mi>α</mi><mrow><mi>B</mi><mi>o</mi><mi>n</mi><mi>f</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>−</mo><mn>0.025</mn><mo>=</mo><mn>0.975</mn></mrow><annotation encoding="application/x-tex">p = 1-\alpha_{Bonf} = 1-0.025 = 0.975</annotation></semantics></math>.</p>
<p>From this it follows that the probability to conduct no error at both
<code>X1</code> and <code>X2</code> is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>α</mi><mrow><mi>B</mi><mi>o</mi><mi>n</mi><mi>f</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mn>0.950625</mn></mrow><annotation encoding="application/x-tex">(1-\alpha_{Bonf})^2 = 0.950625</annotation></semantics></math>.</p>
<p>Now we can calculate the complementary probability of conducting
<em>at least one</em> type I error as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn><mo>−</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>α</mi><mrow><mi>B</mi><mi>o</mi><mi>n</mi><mi>f</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mn>0.049375</mn><mo>=</mo><mn>4.9</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">p = 1 - (1-\alpha_{Bonf})^2 = 0.049375 = 4.9\%</annotation></semantics></math>.
Wait, why is it not 5%? Well, the reason is that we don’t care if one or
both studies produce a type I error, as one error is already enough for
the overall setup to fail. As a result, the Bonferroni correction is not
optimal, as it “wastes” 0.1% of alpha.</p>
<p><br> Of course, this is well-known and a precise alpha adjustment can
be derived via the so-called <a href="https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction" class="external-link">Sidak
correction</a>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mrow><mi>S</mi><mi>i</mi><mi>d</mi><mi>a</mi><mi>k</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>−</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>=</mo><mn>0.02532057</mn><mo>&gt;</mo><msub><mi>α</mi><mrow><mi>B</mi><mi>o</mi><mi>n</mi><mi>f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{Sidak} = 1 - (1-\alpha)^{1/2} = 0.02532057 &gt; \alpha_{Bonf}</annotation></semantics></math>.</p>
<p>Plugging this into the above formula, we get
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn><mo>−</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><msub><mi>α</mi><mrow><mi>S</mi><mi>i</mi><mi>d</mi><mi>a</mi><mi>k</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mn>0.05</mn><mo>=</mo><mn>5</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">p = 1 - (1-\alpha_{Sidak})^2 = 0.05 = 5\%</annotation></semantics></math>.
If you are interested, you can find the derivation of <a href="https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction" class="external-link">Sidak’s</a>
formula under the link, but I have promised you to provide intuitive
explanations, so let’s move to the key concept that will also help to
understand group sequential probabilities.</p>
</div>
<div class="section level3">
<h3 id="combining-two-distributions">Combining two distributions<a class="anchor" aria-label="anchor" href="#combining-two-distributions"></a>
</h3>
<p>If we want to combine both studies, it makes sense to combine the two
density distributions into a two-dimensional (aka bivariate) normal
distribution, which then looks like this.</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-11-1.png" alt="2D-normal-density" width="55%"></p>
<p>Remember that determining the probability of distributions is all
about doing numerical integration under the curve, that is, the overall
type I error now is derived by computing the volume under the blue area.
To do this, we are using the <a href="https://CRAN.R-project.org/package=mvtnorm" class="external-link">mvtnorm</a> package.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;To keep matters simple, we just discuss the results, but
if you want to see the actual R commands, just inspect the source code
of this vignette.&lt;/p&gt;"><sup>1</sup></a></p>
<p>If we plug in the Bonferroni bounds (2.2414) in each dimension and
calculate the probability for the blue area, we get p = 0.049375 while
the Sidak bounds (2.2365) yield p = 0.05, which matches our results from
above.</p>
<p>However, we now also have a graphical intuitive interpretation to
explain the results. For this, have a look at the next graph.</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-13-1.png" alt="2D-normal-density-with-critical-bounds" width="55%"></p>
<p>The grey areas basically mark those scenarios where the type I errors
of both samples fall together. The intuitive explanation now goes like
this: when calculating the overall probability using the two-dimensional
normal distribution, the grey areas are only counted once whereas in the
one-dimensional Bonferroni case you count them twice. To see this, we
can calculate the probability for one of these edges, which is p =
0.00016, and if we subtract them from the overall alpha we get
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.05</mn><mo>−</mo><mn>4</mn><mo>⋅</mo><mn>0.00016</mn><mo>=</mo><mn>0.04936</mn></mrow><annotation encoding="application/x-tex">p = 0.05 - 4\cdot 0.00016 = 0.04936</annotation></semantics></math>,
which corresponds to the overall probability under Bonferroni correction
as we saw above.</p>
<p>So when using the two-dimensional normal distribution, we naturally
derive at the optimal solution for the alpha adjustment. The same is
true for group sequential designs, which further optimize the
calculation by taking into account the dependency of the staged
samples.</p>
</div>
<div class="section level3">
<h3 id="group-sequential-study">Group sequential study<a class="anchor" aria-label="anchor" href="#group-sequential-study"></a>
</h3>
<p>In a (group) sequential study design, samples are analyzed in a
sequence where at each stage all the data from earlier stages are
combined with the data of the current stage.</p>
<p>For example, in the first stage we collect a sample <code>X1</code>
and in a second stage we combine the sample <code>X1</code> with a new
sample <code>X2</code>.</p>
<p><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-14-1.png" alt="group-sequential-study-sample" width="35%" style="display: block; margin: auto;"></p>
<p>The crucial difference when combining the samples in this way is that
now they are inherently dependent, that is, the statistic of sample
<code>X1 + X2</code> will be correlated to the that of sample
<code>X1</code>.</p>
<p>With this in mind, the graphical presentation below is more precise
to visualize the data sample setup.</p>
<div class="figure" style="text-align: center">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-15-1.png" alt="group-sequential-two-stage-study" width="35%"><p class="caption">
Schematic view of group sequential two-stage study.
</p>
</div>
<p>The naive approach for the alpha adjustment again would be to treat
both density distributions as independent and apply the Bonferroni
correction.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-16-1.png" alt="normal-densities-stage-1-and-2" width="85%"><p class="caption">
Normal densities for stage 1 and 2 with Bonferroni bounds.
</p>
</div>
<p>Since this completely ignores the correlation, we expect that even
more alpha is wasted. To see this let’s now again use the
two-dimensional distribution.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-18-1.png" alt="bivariate-normal-densities-of-two-stage-designs" width="30%"><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-18-2.png" alt="bivariate-normal-densities-of-two-stage-designs" width="30%"><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-18-3.png" alt="bivariate-normal-densities-of-two-stage-designs" width="30%"><p class="caption">
Bivariate normal density of two-stage design viewed from different
angles.
</p>
</div>
<p>We again omit the details on how to derive the distribution, but as
you can see, the correlation leads to a flattened bell curve, which in
the independent case before was perfectly symmetrical. Also the amount
of correlation is proportional to the data overlap of the stages. For
example, if the first stage used 5 instead of the 10 data points, the
stages would be less correlated and therefore the bell curve less
flattened.</p>
<p>Based on this distribution, the Bonferroni bounds (2.24) now yield p
= 0.043 so that for this group sequential study 0.7% of alpha would be
“wasted”.</p>
<p>Intuitively, the flattened curve has pulled some mass into the middle
leaving less volume under the blue area.</p>
<div class="section level4">
<h4 id="pocock-design">Pocock design<a class="anchor" aria-label="anchor" href="#pocock-design"></a>
</h4>
<p>The idea of group sequential designs obviously is to come up with
bounds that lead to overall 5% type I error. Let’s start with so-called
Pocock bounds, which means we want equal bounds at all stages. Using the
GroupSeq package, these can be derived as follows.</p>
<p><img src="figures%2Ftwo-stage-Pocock-bounds.png" alt="two-stage-pocock-bounds" width="40%"></p>
<p><img src="figures%2Ftwo-stage-Pocock-bounds-result.png" alt="two-stage-pocock-bounds-result" width="70%"></p>
<p>So the Pocock bounds are 2.1783 and thus as expected lower than the
Bonferroni bounds. The last entry in the ‘cumulative alpha’ column shows
the overall alpha, which is close to 5%.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;In fact, it is closer to 5% (0.0499966086) than shown in
the GroupSeq application, which stems from the fact that the GroupSeq
legacy version uses a different more rudimentary numerical integration
algorithm.&lt;/p&gt;"><sup>2</sup></a></p>
<p>Inspecting the probabilities stage by stage, 2.93% of alpha is spent
at the first and 2.04% at the second stage, respectively. That is,
although the nominal bounds are identical at both stages, due to the
asymmetric shape of the two-dimensional distribution, the resulting
probabilities are different.</p>
<p>The graphical presentation of group sequential designs normally is
done with a graph showing at the x-axis the “Information Rate”, which
marks the relative sample size of the respective stages, and the nominal
bounds at the y-axis. The above Pocock design thus looks something like
this.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-23-1.png" alt="two-stage-pocock-design-with-first-interim-look-at-half" width="50%"><p class="caption">
Group sequential 2-stage Pocock design with one interim look at half of
the total samples.
</p>
</div>
</div>
<div class="section level4">
<h4 id="obrien-fleming-design">O’Brien-Fleming design<a class="anchor" aria-label="anchor" href="#obrien-fleming-design"></a>
</h4>
<p>Probably the most popular group sequential design is the
O’Brien-Fleming design. For our two-stage scenario with half of the
sample used in each stage, it looks like this.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-24-1.png" alt="two-stage-OBF-design" width="50%"><p class="caption">
Group sequential 2-stage O’Brien-Fleming design with one interim look at
0.5.
</p>
</div>
<p>The numbers are outlined below.</p>
<p><img src="figures%2Ftwo-stage-OBF-bounds-result.png" alt="two-stage-OBF-bounds-result" width="70%"></p>
<p>Clearly the critical bounds at stage one are very conservative so
that only 0.3% of alpha is spent there. With spending 4.7% of alpha at
the last stage this O’Brien-Fleming design in fact is not much different
to a standard single stage study, which spends all the 5% on the full
sample. Basically, the first stage is kind of a shot in the dark to
allow catching really big effects.</p>
<p>In contrast, the Pocock design appears to be more “aggressive” trying
harder to terminate the study early. Of course, the drawback of the
Pocock strategy is that if the study cannot be terminated after the
first stage, it then only has about 2% of alpha left for the final
stage.</p>
<p>To apply the O’Brien-Fleming design more aggressively, one could plan
the first stage analysis at a later time point. For example, having the
first interim analysis after 70% of the samples already means that about
1.5% of alpha are spent at stage 1.</p>
</div>
<div class="section level4">
<h4 id="design-comparisons">Design comparisons<a class="anchor" aria-label="anchor" href="#design-comparisons"></a>
</h4>
<p>To strengthen our intuitive understanding, let’s compare the Pocock
and O’Brien-Fleming design in the two-dimensional density view.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-26-1.png" alt="pocock-vs-OBF-design" width="45%"><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-26-2.png" alt="pocock-vs-OBF-design" width="45%"><p class="caption">
Bivariate normal density with critical bounds of Pocock (left) and
O’Brien-Fleming design (right) both with interim analysis after 50% of
the sample.
</p>
</div>
<p>The figure below shows how the time point of the stage 1 analysis
affects the correlation of the stages and with that shapes the design
probabilities.</p>
<div class="figure">
<img src="understanding-gs-designs_files/figure-html/unnamed-chunk-27-1.png" alt="OBF-designs-for-different-t1" width="30%"><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-27-2.png" alt="OBF-designs-for-different-t1" width="30%"><img src="understanding-gs-designs_files/figure-html/unnamed-chunk-27-3.png" alt="OBF-designs-for-different-t1" width="30%"><p class="caption">
Bivariate normal density of two-stage O’Brien-Fleming design with first
stage after 50% (left), 70% (middle) and 90% (right) of all samples.
</p>
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://rpahl.github.io/r-some-blog/about.html" class="external-link">Roman Pahl</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
