---
title: "Introduction to GroupSeq"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 4
description: >
  Start here if this is your first time using GroupSeq. This vignette
  gives a brief overview on *what* and *what not* can be done with GroupSeq.
vignette: >
  %\VignetteIndexEntry{Introduction to GroupSeq}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-setup, include = FALSE}
library(knitr)
library("ggplot2")

knitr::opts_chunk$set(
  comment = "#",
  prompt = F,
  tidy = FALSE,
  cache = FALSE,
  collapse = T,
  echo = FALSE,
  dpi = 300,
  fig.width = 5, fig.height = 5
)

okabe_palette <- c('orange' = "#E69F00",
                   'sky blue' = "#56B4E9",
                   'bluish green' = "#009E73",
                   'yellow' = "#F0E442",
                   'blue' = "#0072B2",
                   'vermillion' = "#D55E00",
                   'reddish purple' = "#CC79A7")

options(width = 100L, digits = 10)
```

# Note: this vignette is still under construction

## How to calculate group sequential design probabilities

While calculating probabilities of group sequential designs can be considered
somewhat complex, it may be less difficult than most people think. This vignette
provides a visual explanation of the underlying method to help forming 
an intuitive understanding of this topic.
The author is convinced that gaining an intuitive understanding of these 
calculations helps when applying and interpreting such designs in practice.

### One-sample study
Following the easy-to-hard principle, we start with conventional non-sequential
study designs and gradually progress to the group sequential case.

Consider you have a sample of data, let's call it `X1`.

```{r}
fill_colors = c(X1 = okabe_palette[["bluish green"]],
                X2 = okabe_palette[["vermillion"]],
                "X1+X2" = "#6B7E3A")

data  = data.frame(x = c(1:10, 1:10, 1:10, 1:20),
                   y = c(rep(2, 10), rep( 1.5, 10),
                         rep(0, 10), rep(-0.5, 20)),
                   Sample = c(rep("X1", 10), rep("X2", 10),
                              rep("X1", 20), rep("X2", 10))
)
```


```{r, out.width = "35%", message = FALSE, fig.height = 1, fig.width = 2.5, fig.align = "center"}
dat = data[1:10, ]
ggplot(dat, aes(x = x, y = y)) +
    geom_point(size = 10, shape = 21, aes(fill = Sample)) +
    scale_fill_manual(values = fill_colors[1]) +
    lims(y = c(2, 2)) +
    theme_void() +
    theme(legend.position = "bottom")

```
<br>
Next you calculate a standardized test statistic. Assuming a standard normal
distribution, the density distribution under the null hypothesis forms the 
well-known bell-shaped curve.

```{r, echo = FALSE, out.width = "65%", fig.cap = fig.cap, fig.height=3}
fig.cap = "Standard Normal Density with critial bounds marking 5% significance level."

blue = okabe_palette[["sky blue"]]
yellow = okabe_palette[["yellow"]]


crit = qnorm(1 - 0.025)
probLabel = paste0(pnorm(-crit)*100, "%")
x = seq(-3, 3, by = 0.01)

normalDensPlot = 
    ggplot(data.frame(x = x), aes(x = x)) +
    stat_function(fun = dnorm, geom = "area", fill = blue, xlim = c(-4, -crit)) +
    stat_function(fun = dnorm, geom = "area", fill = yellow, xlim = c(-crit, crit)) +
    stat_function(fun = dnorm, geom = "area", fill = blue, xlim = c(crit, 4)) +
    stat_function(fun = dnorm, color = okabe_palette[["bluish green"]], size = 1.2) +
    geom_segment(x = 2.2, xend = 2.7, y = 0.02, yend = 0.07) +
    geom_segment(x = -2.2, xend = -2.7, y = 0.02, yend = 0.07) +
    annotate("text", label = probLabel, x = 2.9, y = 0.09, size = 3.5) +
    annotate("text", label = probLabel, x = -2.9, y = 0.09, size = 3.5) +
    lims(x = range(x)) +
    labs(x = expression(x[1]), 
         y = expression(f(x[1]))) +
    theme_minimal() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          axis.ticks.x = element_line())

normalDensPlot
```

<br>
If a two-sided test is planned with a significance level of $\alpha = 5\%$, 
the critical bounds are set as visualized in the above figure. 
In the days before computers these bounds had to be looked up in tables in some
book, but these days of course you just plug-in the numbers in your favorite
statistics program. In R its
```{r, echo = TRUE}
qnorm(0.025)
```

Now what do these bounds mean? Basically, if the null
hypothesis ($H_0$) was true, the probability of obtaining a test statistic at or
beyond these bounds is $2\cdot 0.025 = 0.05$, which therefore 
limits the so-called type I error (rejecting $H_0$ although its true) at 5%.

Going forward the key thing to notice is that calculating this probability 
basically means *calculating the (blue) area under the density curve* (see above
figure). Again, these days it's easy to derive the area using computer's 
numerical integration.


### Combining two studies

Now lets consider another sample `X2` that was collected independently from `X1`.


```{r, out.width = "35%", message = FALSE, fig.height = 1.3, fig.width = 2.5, fig.align = "center"}
dat = data[1:20, ]
ggplot(dat, aes(x = x, y = y)) +
    geom_point(size = 10, shape = 21, aes(fill = Sample)) +
    scale_fill_manual(values = fill_colors[1:2]) +
    lims(y = c(1.2, 2.2)) +
    theme_void() +
    theme(legend.position = "bottom")

```

<br>
If we construct a distribution similar to the one above, we have the following.

```{r, fig.width = 8, fig.height = 3, out.width = "85%"}
normalDensPlotX2 = normalDensPlot + 
    labs(x = expression(x[2]), y = expression(f(x[2]))) +
    stat_function(fun = dnorm, color = okabe_palette[["vermillion"]], size = 1.2)
gridExtra::grid.arrange(normalDensPlot, normalDensPlotX2, nrow = 1)
```

<br>
A statistical test on both samples now would control the type I error for each of 
these studies at the 5% level, but, of course, things are more complicated
if you consider both samples at the same time, because now you have two chances
of conducting the error.

So if we assume our samples `X1` and `X2` were sampled from
the same population and we are interested to test the same outcome,
to control the *overall* type I error at 5% we need to down-adjust alpha.
The simplest and most common approach is to use the Bonferroni correction,
which means we are allowed to spend $\alpha_B = \alpha/2 = 0.025$.
The corresponding critical bound is 
```{r, echo = TRUE}
qnorm(0.025/2)
```
and we have

```{r, echo = FALSE, out.width = "65%", fig.cap = fig.cap, fig.height=3}
fig.cap = "Standard Normal Density with critial bounds marking 5% significance level."

blue = okabe_palette[["sky blue"]]
yellow = okabe_palette[["yellow"]]

crit = qnorm(1 - 0.025/2)
probLabel = paste0(pnorm(-crit)*100, "%")
x = seq(-3, 3, by = 0.01)

normalDensPlotAdj = 
    ggplot(data.frame(x = x), aes(x = x)) +
    stat_function(fun = dnorm, geom = "area", fill = blue, xlim = c(-4, -crit)) +
    stat_function(fun = dnorm, geom = "area", fill = yellow, xlim = c(-crit, crit)) +
    stat_function(fun = dnorm, geom = "area", fill = blue, xlim = c(crit, 4)) +
    stat_function(fun = dnorm, color = okabe_palette[["bluish green"]], size = 1.2) +
    geom_segment(x = 2.2+.2, xend = 2.7+.2, y = 0.02, yend = 0.07) +
    geom_segment(x = -2.2-.2, xend = -2.7-.2, y = 0.02, yend = 0.07) +
    annotate("text", label = probLabel, x = 2.9, y = 0.09, size = 3.5) +
    annotate("text", label = probLabel, x = -2.9, y = 0.09, size = 3.5) +
    lims(x = range(x)) +
    labs(x = expression(x[1]), 
         y = expression(f(x[1]))) +
    theme_minimal() +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          axis.ticks.x = element_line())
```

```{r, fig.width = 8, fig.height = 3, out.width = "85%"}
normalDensPlotX2Adj = normalDensPlotAdj + 
    labs(x = expression(x[2]), y = expression(f(x[2]))) +
    stat_function(fun = dnorm, color = okabe_palette[["vermillion"]], size = 1.2)
gridExtra::grid.arrange(normalDensPlotAdj, normalDensPlotX2Adj, nrow = 1)
```


```{r}
alpha = 0.05
alpha.Bonf = alpha / 2
alpha.Sidak = 1 - (1-alpha)^(1/2)

crit.Bonf = -rep(qnorm(alpha.Bonf / 2), 2)
crit.Sidak = -rep(qnorm(alpha.Sidak / 2), 2)
```
So if we calculate the overall type I error probability, we sum up all the areas
and get $p = 1.25\% + 1.25\%+ 1.25\% + 1.25\% = 5\%$.
So we are good, are we? Well, the devil is in the details. 

Remember, the goal was to make
sure that we don't conduct a type I error. The probability to conduct no
type I at either `X1` or `X2` is $1-\alpha_B = 1-0.025 = 0.975$. 

From this it
follows that the probability to conduct no error at both `X1` and `X2` is
$(1-\alpha_B)^2 = 0.950625$. 

Now we can calculate the complementary probability
of conducting *at least one* type I error* as 
$1 - (1-\alpha_B)^2 = 0.049375 = 4.9\%$. Wait, why is this not 5%? The reason
is that we don't care if one or both studies produce a type I error, as one
error is already enough for the overall setup to fail. For this reason the
Bonferroni correction is not optimal, as it "wastes" 0.1% of alpha. 

<br>
Of course, this is well-known and a precise alpha adjustment can be derived via 
the so-called
[Sidak correction](https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction):
$\alpha_S* = 1 - (1-\alpha)^{1/2} = 0.02532057 > \alpha_B$.

Plugging this into the above formula, we get
$1 - (1-\alpha_S)^2 = 0.05 = 5\%$. If you are interested, you can find the
derivation of [Sidak's](https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction) formula
under the link, but I have promised you to provide intuitive explanations,
so let's move to the key concept that will help to understand group sequential
probabilities.


### Combining two distributions

Remember that determining the probability of distributions is all about
calculating areas under the curve. Fortunately, two normal distributions
can be combined into a bivariate normal distribution, which then looks
like this.

```{r prepare-2D-normal-density-plot, echo = FALSE, include = FALSE}
mu = 0
Mean = c(mu, mu)
covariance = 0
Sigma0 = matrix(c(1, rep(covariance, 2), 1), 2)


width = 6.6
binwidth = 0.2
x1 = x2 = seq(from = mu - width/2, to = mu + width/2, by = binwidth)
grid = expand.grid(x1, x2)

z = mvtnorm::dmvnorm(grid, mean = Mean, sigma = Sigma0)
zmat = matrix(z, ncol = length(x1))

grid.col = expand.grid(x1[-1] - binwidth/2, x2[-1] - binwidth/2)
gc1 = grid.col[, 1]
gc2 = grid.col[, 2]

crit = crit.Sidak
get_color = Vectorize(function(x1, x2) {
    if (x1 < -crit[1] || x1 > crit[1] || x2 < -crit[2] || x2 > crit[2])
        blue else yellow
})

plot_normal_dens_2d = function(zmat, ...) {
    persp(x1, x2, zmat,
          expand = 0.5,
          lphi = 0,
          xlab = "x1",
          ylab = "x2\n",
          zlab = "f(x1, x2)",
          shade = 0.2,
          ticktype = "detailed",
          ...)
}
```


```{r, out.width = "55%"}
op = par(mar = c(0, 2, 0, 0))
plot_normal_dens_2d(zmat, phi = 45, col = get_color(gc1, gc2))
par(op)
```

Now to derive the overall type I error, we again just have to compute the 
area under the blue grid. To do this, we are using the
[mvtnorm](https://CRAN.R-project.org/package=mvtnorm). To keep matters simple,
we just discuss the results. For further details you can see
the source code of this vignette.


```{r, include = FALSE}
crit.Bonf = -rep(qnorm(alpha.Bonf / 2), 2)
crit.Sidak = -rep(qnorm(alpha.Sidak / 2), 2)

crit = crit.Bonf
crit = crit.Sidak

calc2Dprob = function(crit, sigma) {
    calc_prob = function(lower, upper) {
        as.numeric(mvtnorm::pmvnorm(lower = lower, upper = upper, sigma = sigma))
    }
        
    left_stripe = calc_prob(lower = c(-Inf, -Inf), upper = c(-crit[1], Inf))
    lower_rectangle = calc_prob(lower = c(-crit[1], -Inf), upper = c(crit[1], -crit[2]))
    2 * left_stripe + 2 * lower_rectangle # return two-sided prob
}

p.edge = as.numeric(mvtnorm::pmvnorm(lower = c(-Inf, -Inf), 
                                     upper = c(-crit[1], -crit[2]), 
                                     sigma = Sigma0))


p2D.Bonf = calc2Dprob(crit.Bonf, sigma = Sigma0)
p2D.Sidak = calc2Dprob(crit.Sidak, sigma = Sigma0)
```

Now if we plug in the Bonferoni bounds, the calculation of the blue area,
results in p = `r p2D.Bonf` while the Sidak bounds yield p = `r p2D.Sidak`,
which therefore confirms the above results.

However, we now also have a graphical intuitive interpretation to explain the
results. For this, have a look at the next graph.

```{r, out.width = "55%"}
orange = okabe_palette[["orange"]]
crit = crit.Sidak
get_color_edges = Vectorize(function(x1, x2) {
    # Make edges orange
    if (x1 < -crit[1] && x2 < -crit[2] ||
        x1 < -crit[1] && x2 >  crit[2] ||
        x1 >  crit[1] && x2 < -crit[2] ||
        x1 >  crit[1] && x2 >  crit[2])
        return("grey")
    
    if (x1 < -crit[1] || x1 > crit[1] || x2 < -crit[2] || x2 > crit[2])
        blue else yellow
})

op = par(mar = c(0, 2, 0, 0))
plot_normal_dens_2d(zmat, phi = 45, col = get_color_edges(gc1, gc2))
par(op)
```


The grey areas mark those scenarios where both type I errors fall together.
The intuitive explanation now goes like this: when calculating the overall area,
the grey areas are only counted once whereas in the Bonferroni case you
count them twice. To see this, we can calculate the area of one of these edges,
which results in p = 0.00016 and if we subtract them from the overall alpha
we get $0.05 - 4\cdot 0.00016 = 0.04936$, which corresponds to the overall 
probability under Bonferroni correction.

So when using the bivariate normal distribution, we naturally derive at the
optimal solution for the alpha adjustment. The same is true for group sequential
designs, which further optimize the calculation by taking into account
the dependency of the staged samples.


### Two-stage designs

```{r, echo = FALSE, out.width = "50%", fig.show="hold", fig.cap = "Group sequential 2-stage design with one interim look at 0.5 and O'Brien-Fleming type alpha spending."}

asOBF <- function(t, alpha = 0.05, side = 2) {
  2 * (1 - stats::pnorm((stats::qnorm(1 - (alpha / side)/2)) / sqrt(t)))
}

n = 100
tt = c(.5, 1)
crit = -qnorm(asOBF(tt))
x = seq(0.2, 1.0, length = n)

upperLine = data.frame(x = x, y = -qnorm(asOBF(x)))
lowerLine = data.frame(x = x, y = qnorm(asOBF(x)))

upperArea = data.frame(x = x, ymin = upperLine$y, ymax = 6, y = 3)
innerArea = data.frame(x = x, ymin = lowerLine$y, ymax = upperLine$y, y = 0)
lowerArea = data.frame(x = x, ymin = -6, ymax = lowerLine$y, y = -3)

p = ggplot(mapping = aes(x = x, y = y)) +

    geom_ribbon(data = upperArea, aes(x = x, ymin = ymin, ymax = ymax, y = y), fill = blue) +
    geom_ribbon(data = innerArea, aes(x = x, ymin = ymin, ymax = ymax, y = y), fill = yellow) +
    geom_ribbon(data = lowerArea, aes(x = x, ymin = ymin, ymax = ymax, y = y), fill = blue) +

    geom_line(data = upperLine) +
    geom_line(data = lowerLine) +

    geom_point(data = data.frame(x = tt, y =  crit), size = 5, shape = 21, fill = "grey") +
    geom_point(data = data.frame(x = tt, y = -crit), size = 5, shape = 21, fill = "grey") +

    theme_minimal() +
    labs(x = "Information Rate", y = "Critical Value") +
    lims(y = c(-6, 6)) +
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          axis.ticks.x = element_line())

p

```


```{r, fig.show="hold", out.width = "75%", fig.cap = "Bivariate normal density of two-stage design."}

mu = 0
Mean = c(mu, mu)
covariance = sqrt(0.5)
Sigma = matrix(c(1, rep(covariance, 2), 1), 2)

z = mvtnorm::dmvnorm(grid, mean = Mean, sigma = Sigma)
zmat = matrix(z, ncol = length(x1))

op = par(mar = rep(2.2, 4))
plot_normal_dens_2d(zmat, phi = 15, col = get_color(gc1, gc2))
par(op)
```

```{r, fig.show="hold", out.width = "30%", fig.cap = "Bivariate normal density of two-stage design."}
op = par(mar = rep(.1, 4))
plot_normal_dens_2d(zmat, phi = 40, col = get_color(gc1, gc2), box = FALSE)
plot_normal_dens_2d(zmat, phi = 65, col = get_color(gc1, gc2), box = FALSE)
plot_normal_dens_2d(zmat, phi = 90, col = get_color(gc1, gc2), box = FALSE)
par(op)
```



### Probability distribution of group sequential designs

```{r}
mu = 0
Mean = c(mu, mu)
covariance = sqrt(0.5)
Sigma = matrix(c(1, rep(covariance, 2), 1), 2)

z = mvtnorm::dmvnorm(grid, mean = Mean, sigma = Sigma)
zmat = matrix(z, ncol = length(x1))
```


```{r, fig.show="hold", out.width = "45%", fig.cap = "Bivariate normal density of two-stage design."}
op = par(mar = rep(1.5, 4))
plot_normal_dens_2d(zmat, phi = 45, col = get_color(gc1, gc2))
plot_normal_dens_2d(zmat, phi = 90, col = get_color(gc1, gc2))
par(op)
```


## Basic usage

Load the library to start the graphical user interface.
```{r, eval = FALSE}
library("GroupSeq")
```

```{r, out.width = "30%", echo = FALSE}
include_graphics("figures/menu-after-load.png")
```

<br>

Now the application splits into four possible tasks.

1. [Compute Bounds](https://rpahl.github.io/GroupSeq/articles/1-compute-bounds-H0.html)
2. [Compute Drift given Power and Bounds](https://rpahl.github.io/GroupSeq/articles/2-compute-drift.html)
3. [Compute Probabilities given Bounds and Drift](https://rpahl.github.io/GroupSeq/articles/3-compute-bounds-H1.html)
4. [Compute Confidence Interval](https://rpahl.github.io/GroupSeq/articles/4-compute-CI.html)

To select a task, you have to hit the `Perform Selected Task` button.
